#!/goinfre/alvgomez/miniconda3/envs/42cyber-alvgomez/bin/python
# **************************************************************************** #
#                                                                              #
#                                                         :::      ::::::::    #
#    spider                                             :+:      :+:    :+:    #
#                                                     +:+ +:+         +:+      #
#    By: alvgomez <alvgomez@student.42.fr>          +#+  +:+       +#+         #
#                                                 +#+#+#+#+#+   +#+            #
#    Created: 2023/04/11 13:50:20 by alvgomez          #+#    #+#              #
#    Updated: 2023/04/17 15:45:17 by alvgomez         ###   ########.fr        #
#                                                                              #
# **************************************************************************** #

import requests
import argparse
from bs4 import BeautifulSoup
import shutil
import os
import sys

all_images = []
all_links = []
pictures = []

def get_img_urls(url):
    images = []
    try:
        file = open(url, "r").read()
        soup = BeautifulSoup(file, "html.parser")
    except:
        try:
            page = requests.get(url)
            soup = BeautifulSoup(page.content, "html.parser")
        except:
            return images
    for img in soup.findAll('img'):
        try:
            if (img.get('src')) not in all_images:
                images.append(img.get('src'))
                all_images.append(img.get('src'))
        except:
            pass
    return images

def picture_format(img):
    formats = [".jpg", ".jpeg", ".png", ".gif", ".bmp"]
    for ext in formats:
        if ext in img:
            return ext
    return None
        
def get_img_into_folder(images, url, file="./data", round=0):
    if not os.path.exists(file):
        os.makedirs(file)
    i = 0
    for img in images:
        try:
            obj = None
            if os.path.exists(url) and os.path.exists(img):
                obj = open(img, "rb")
            else:
                req = requests.get(img, stream = True)
                if req.status_code == 200:
                    obj = req.raw
                else:
                    obj = None
            format = picture_format(img)
            if format and obj:
                for item in pictures:
                    if f"pic_{i}" in item:
                        i += 1
                src = os.path.join(file, (f"pic_{i}" + format))
                with open(src, 'wb') as f:
                    shutil.copyfileobj(obj, f)
                    pictures.append(src)
        except:
            pass
    
def get_links(url, first_url):
    links = []
    try:
        file = open(url, "r").read()
        soup = BeautifulSoup(file, "html.parser")
    except:
        try:
            page = requests.get(url)
            soup = BeautifulSoup(page.content, "html.parser")
        except:
            return links
    i = 0
    for link in soup.findAll('a', href=True):
        try:
            if first_url in link.get('href') or os.path.dirname(first_url) in link.get('href'):
                links.append(link.get('href'))
                i += 1
        except:
            pass
    return links
    
def recursive_download(link=None, first_url=None, depth=5, round=0, file="./data"):
    if round >= depth:
        pass
    else: 
        all_links.append(link)
        images = get_img_urls(link)
        get_img_into_folder(images, link, file, round)
        links = get_links(link, first_url)
        round += 1
        if links:
            for url in links:
                if url not in all_links:
                    recursive_download(url, first_url, depth, round, file)  
     
def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument('link', type=str, action="store", help="The url of the site to scrap")
    parser.add_argument('-r', '--recursive', action="store_true", help="Recursive option")
    parser.add_argument('-l', '--depth', type=int, action="store", help="When recursive option is on, select level of depth")
    parser.add_argument('-p', '--path', type=str, action="store", help="Indicates the path where to download files")
    arg = parser.parse_args()
    if arg.depth and not arg.recursive:
        raise parser.error("-r is required for -l option")
    if arg.recursive is not True:
        arg.depth = 1
    if not arg.depth:
        arg.depth = 5
    if not arg.path:
        arg.path = "./data"
    return arg

if __name__ == "__main__":
    arg = parse_arguments()
    recursive_download(link=arg.link, first_url=arg.link, depth=arg.depth, file=arg.path)

    